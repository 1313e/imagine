{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGINE tutorial 1\n",
    "\n",
    "### TestField + LiSimulator + Multinest\n",
    "\n",
    "in this tutorial, we focus on introducing the basic building blocks in IMAGINE package and how to use them for assembling a Bayesian analysis pipeline\n",
    "\n",
    "- the target field in play is designed to vaively mimic Faraday depth, which is affected linearly by (Galactic) magnetic field\n",
    "\n",
    "$ field(x) = cos(x) * \\mathcal{G}(mean=a,std=b;seed=s) $, $x \\in (0,2\\pi)$\n",
    "\n",
    "where $\\{a,b\\}$ is the 'physical' parameter set, $s$ represents the seed for random variable generation\n",
    "\n",
    "- first of all we need to prepare mock data, which should contain the signal field and a noise field\n",
    "\n",
    "- with mock data and its (co)variance matrix, we shall assemble IMAGINE pipeline and execute for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import logging as log\n",
    "\n",
    "# visualize posterior\n",
    "import corner\n",
    "import json\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from imagine.tools.carrier_mapper import unity_mapper\n",
    "\n",
    "from imagine.observables.observable_dict import Simulations, Measurements, Covariances\n",
    "from imagine.likelihoods.ensemble_likelihood import EnsembleLikelihood\n",
    "from imagine.fields.test_field.test_field_factory import TestFieldFactory\n",
    "from imagine.priors.flat_prior import FlatPrior\n",
    "from imagine.simulators.test.li_simulator import LiSimulator\n",
    "from imagine.pipelines.multinest_pipeline import MultinestPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1, mock data preparation\n",
    "\n",
    "the basic logic in mock data preparation reads\n",
    "\n",
    "$ data(x) = signal(x) + noise(x) $\n",
    "\n",
    "for simplicity, we propose a gaussian noise\n",
    "\n",
    "$ noise(x) = \\mathcal{G}(mean=0,std=e) $ whose seed is irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = 3. # true value of a\n",
    "b0 = 6. # true value of b\n",
    "e = 0.1 # std of gaussian measurement error\n",
    "s = 233 # seed fixed for signal field\n",
    "\n",
    "size = 10 # data size in measurements\n",
    "x = np.linspace(0,2.*np.pi,size) # where the observer is looking at\n",
    "\n",
    "np.random.seed(s) # set seed for signal field\n",
    "\n",
    "signal = np.multiply (np.cos(x), np.random.normal(loc=a0,scale=b0,size=size))\n",
    "\n",
    "data = np.vstack([signal + np.random.normal(loc=0.,scale=e,size=size)]) # vstack will be explained later\n",
    "\n",
    "cov = (s**2) * np.eye(size) # pre-defined according to measurement error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMAGINE will not take raw data assembled above directly\n",
    "\n",
    "the reason is this: there will be many data coming at the same time, with various formats and sizes\n",
    "\n",
    "IMAGINE has to register different data by name, format, size, etc., in order to trigger simulator efficiently\n",
    "\n",
    "so, we designed **ObservableDict** class for help\n",
    "\n",
    "**ObservableDict** class is build upon class **Observable** which is supported by **NIFTy5** at the backend\n",
    "\n",
    "as it is named, **ObservableDict** hosts a dictionary for **Observable** objects which host fields or ensemble of fields\n",
    "\n",
    "as you may have realized, **ObservableDict** is not only convenient for hosting data, but also available to outputs from simulators\n",
    "\n",
    "so under this base class, come three children \n",
    "\n",
    "**Measurements** class for observational/measured data sets\n",
    "\n",
    "**Simulations** class for simulated outputs\n",
    "\n",
    "**Covariances** class for observational/measured data covariance matrices\n",
    "\n",
    "data and/or covariance matrix should be firstly formed in either numpy ndarray type or NIFTy Field type\n",
    "\n",
    "a tag should be applied to each data/matrix piece, with convention:\n",
    "\n",
    "**( 'name', 'observed frequency in GHz', 'size or HEALPix Nside', 'extra tag' )** \n",
    "\n",
    "i.e., a WMAP 23GHz STOKES Q map with Nside 128 may have a dictionary entry ('wmap','23','128','Q')\n",
    "\n",
    "another example, a plain test map with size 56 may have a dictionary entry ('test','nan','56','nan')\n",
    "\n",
    "there, if no frequency or extra tag is necessary fill the blanck with 'nan'\n",
    "\n",
    "to collect data/matrix, use **append** function as illustrated below, notice that if a plain data is about to be \n",
    "\n",
    "appended, set plain=True since the default argument is for HEALPix map with 'Nside' in its entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = Measurements() # create empty Measrurements object\n",
    "mock_data.append(('test', 'nan', str(size), 'nan'), data, True)\n",
    "\n",
    "mock_cov = Covariances() # create empty Covariance object\n",
    "mock_cov.append(('test', 'nan', str(size), 'nan'), cov, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is a reason we use numpy vstack in the very beginning\n",
    "\n",
    "in IMAGINE we decide not to explicitly use numpy vector with shape (data size, )\n",
    "\n",
    "instead we force the convention to be (ensemble size, data size)\n",
    "\n",
    "now if you are familiar to NIFTy5, it is straight forwad in knowing retriving raw data from above\n",
    "\n",
    "nevertheless, we show it here, in **Observable** class we aligned the to_global_data the same as which in NIFTy5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.plot(x, mock_data[('test', 'nan', str(size), 'nan')].to_global_data()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2, pipeline assembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after mock data, there are several steps to the final pipeline:\n",
    "\n",
    "we need to know likelihood and priors, field models and simulators\n",
    "\n",
    "let's do field models and simulators first\n",
    "\n",
    "simulators are merely python interfaces to external libraries, i.e., hammurabi X\n",
    "\n",
    "there will be cases where users want to bind several external libraries together\n",
    "\n",
    "for that we suggest users to assemble there own interface, rather than use several existed simulator interfaces\n",
    "\n",
    "it will become clear when you see field and factry classes\n",
    "\n",
    "for physical fields, we propose **GeneralFieldFactory** and **GeneralField** classes as templates\n",
    "\n",
    "factory classes will be directly invoked in pipeline, in order to generate field objects\n",
    "\n",
    "field objects once being generated, will be hand in to simulators for producing observables\n",
    "\n",
    "the reason for seperately defining factories and fields lies in the fact that\n",
    "\n",
    "in each random walk of Bayesian analysis, we update active parameters in fields and unmistakably send them to simulators\n",
    "\n",
    "factories are like 'factories', users have to define active parameters and whose varying ranges\n",
    "\n",
    "updating field instant parameter values is also done in factories\n",
    "\n",
    "fields, on the other hand, does nothing but informs simulators the latest parameter values and where to find them\n",
    "\n",
    "so a particularily assembled simulator may very likely be different from others\n",
    "\n",
    "especially in where and how parameters are registered\n",
    "\n",
    "we recommend XML format for all simulators\n",
    "\n",
    "yep, neither fields nor factories contain the actual method for GENERATING field scalars/vectors\n",
    "\n",
    "we though it is not necessary to do so (at least currently) as simulators will take care of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = TestFieldFactory(active_parameters=('a','b')) # factory with single active parameter\n",
    "factory.parameter_ranges = {'a':(0,10),'b':(0,10)} # adjust parameter range for Bayesian analysis\n",
    "factory_list = [factory] # a list of factories is required by pipeline\n",
    "\n",
    "simer = LiSimulator(mock_data) # simulator needs to know what observables it should provide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now comes the priors and likelihoods\n",
    "\n",
    "IMAGINE provides **Likelihood** class with **EnsembleLikelihood** and **SimpleLikelihood** as two working children\n",
    "\n",
    "we do not have space to introduce how log-likelihoods are calculated here\n",
    "\n",
    "likelihood initialization requires measurements, this is at the front end \n",
    "\n",
    "while driving it requires simulated outputs, this is prisoned inside pipelines at the back end\n",
    "\n",
    "it is considered that in many cases, not all data sets are accompanied by covarainces\n",
    "\n",
    "**Likelihood** is flexible about that so no worries\n",
    "\n",
    "what to do if there is not a single covariance matrix?\n",
    "\n",
    "users can initialize likelihood object with data only (**Measurements** object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = FlatPrior()\n",
    "\n",
    "likelihood = EnsembleLikelihood(mock_data, mock_cov) # initialize likelihood with measured info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, the pipeline at last!\n",
    "\n",
    "as you will see below, **Pipeline** class takes all above stuff to take off\n",
    "\n",
    "basice input arguments are presented\n",
    "\n",
    "sure, there are tons of controlling parameters associated to Bayesian samplers\n",
    "\n",
    "for that, users have to dig into documentations of samplers and our source code\n",
    "\n",
    "we have a few instructions there\n",
    "\n",
    "anyway, we provide **MultinestPipeline** and **DynestyPipeline**\n",
    "\n",
    "**Multinest** or **pyMultinest** is quite old but fast, as its core in Fortran\n",
    "\n",
    "**Dynesty** is new and purely in Python, extremely well documented, but it's slooooow, well not too slow\n",
    "\n",
    "interestingly, Dynesty and pyMultinest share similar frontend so no big trouble in shifting\n",
    "\n",
    "read and paly with the examples then you will understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size = 100\n",
    "pipe = MultinestPipeline(simer, factory_list, likelihood, prior, ensemble_size)\n",
    "pipe.random_seed = 0 # favor fixed seed? try a positive integer\n",
    "pipe.pymultinest_parameter_dict = {'n_iter_before_update': 1,\n",
    "                                   'n_live_points': 400,\n",
    "                                   'verbose': True,\n",
    "                                   'resume': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autobots, roll out\n",
    "\n",
    "IMAGINE pipeline use variable cubes with range $[0,1]$ as it is\n",
    "\n",
    "converting from variables to parameters are automatically done in factories\n",
    "\n",
    "for visualizing results we need a bit detour but easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipe()\n",
    "\n",
    "samples = results['samples']  # load sample points\n",
    "\n",
    "for i in range(len(pipe.active_parameters)): # convert variables into parameters\n",
    "    low, high = pipe.active_ranges[pipe.active_parameters[i]]\n",
    "    for j in range(samples.shape[0]):\n",
    "        samples[j,i] = unity_mapper(samples[j,i],low,high)\n",
    "\n",
    "# corner plot\n",
    "figure = corner.corner(samples[:, :len(pipe.active_parameters)],\n",
    "                       range=[0.99]*len(pipe.active_parameters),\n",
    "                       quantiles=[0.02, 0.5, 0.98],\n",
    "                       labels=pipe.active_parameters,\n",
    "                       show_titles=True,\n",
    "                       title_kwargs={\"fontsize\": 15},\n",
    "                       color='steelblue',\n",
    "                       truths=[a0,b0],\n",
    "                       truth_color='firebrick',\n",
    "                       plot_contours=True,\n",
    "                       hist_kwargs={'linewidth': 2},\n",
    "                       label_kwargs={'fontsize': 15})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
